# -*- coding: utf-8 -*-
"""MiniP1_1_Q2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iQP0V7t7a3Epjyh_TGLir-5sJ6rnuA6S

PART 2:

Q1:
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification

#https://drive.google.com/file/d/116ho57xYxPgjTET5qOR30VLfwoxFAaI4/view?usp=sharing

!pip install --upgrade --no-cache-dir gdown
!gdown 116ho57xYxPgjTET5qOR30VLfwoxFAaI4

df = pd.read_csv('/content/data_banknote_authentication.txt', delimiter=',')
df.to_csv('/content/data_banknote_authentication.csv', index=False)

"""Q2:"""

shuffled_df = df.sample(frac=1, random_state=13).reset_index(drop=True)

from sklearn.model_selection import train_test_split

X = shuffled_df[['Variance', 'Skewness', 'kurtosis', 'Entropy']].values
y = shuffled_df[['Class ']].values

x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=13, test_size=0.2)
x_train.shape, x_test.shape, y_train.shape, y_test.shape,

"""Q3:

Logistic Regression Model
"""

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def logistic_regression(x, w):
    y_hat = sigmoid(x @ w)
    return y_hat

"""Binary Cross Entropy(BCE)"""

def bce(y, y_hat):
    loss = -(np.mean(y*np.log(y_hat) + (1-y)*np.log(1-y_hat)))
    return loss

"""Gradient"""

def gradient(x, y, y_hat):
    grads = (x.T @ (y_hat - y)) / len(y)
    return grads

"""Gradient Descent"""

def gradient_descent(w, eta, grads):
    w -= eta*grads
    return w

"""Accuracy"""

def accuracy(y, y_hat):
    acc = np.sum(y == np.round(y_hat)) / len(y)
    return acc

"""Train"""

x_train = np.hstack((np.ones((len(x_train), 1)), x_train))
x_train.shape

m = 4
w = np.random.randn(m+1, 1)
print(w.shape)

eta = 0.01
n_epochs = 2000

"""Error History"""

error_hist = []

for epoch in range(n_epochs):
    # predictions
    y_hat = logistic_regression(x_train, w)

    # loss
    e = bce(y_train, y_hat)
    error_hist.append(e)

    # gradients
    grads = gradient(x_train, y_train, y_hat)

    # gradient decsent
    w = gradient_descent(w, eta, grads)

    if (epoch+1) % 100 == 0:
        print(f'Epoch={epoch}, \t E={e:,.4},\t w+{w.T[0]}')

"""Plot Error"""

plt.plot(error_hist)

"""Test"""

x_test = np.hstack((np.ones((len(x_test), 1)), x_test))
x_test.shape

y_hat = logistic_regression(x_test, w)
accuracy(y_test, y_hat)

"""Q4 & Q5:"""

from sklearn.preprocessing import StandardScaler
data2 = pd.read_csv('/content/data_banknote_authentication.csv')

scaler = StandardScaler()

scaled_data2 = scaler.fit_transform(data2)

scaled_df2 = pd.DataFrame(scaled_data2, columns=data2.columns)

shuffled_scaled_df2 = scaled_df2.sample(frac=1, random_state=13).reset_index(drop=True)

X2 = shuffled_scaled_df2[['Variance', 'Skewness', 'kurtosis', 'Entropy']].values
y2 = shuffled_scaled_df2[['Class ']].values

x_train2, x_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.2)

x_train2 = np.hstack((np.ones((len(x_train2), 1)), x_train2))

m_new = 4
w_new = np.random.randn(m_new+1, 1)
print(w_new.shape)

eta_new = 0.01
n_epochs_new = 2000

error_hist_new = []

for epoch_new in range(n_epochs_new):
    # predictions
    y_hat2 = logistic_regression(x_train2, w_new)

    # loss
    e_new = bce(y_train2, y_hat2)
    error_hist_new.append(e_new)

    # gradients
    grads_new = gradient(x_train2, y_train2, y_hat2)

    # gradient decsent
    w_new = gradient_descent(w_new, eta_new, grads_new)

    if (epoch_new+1) % 100 == 0:
        print(f'Epoch={epoch_new}, \t E={e_new:,.4},\t w={w_new.T[0]}')

plt.plot(error_hist_new)

x_test2 = np.hstack((np.ones((len(x_test2), 1)), x_test2))

y_hat2 = logistic_regression(x_test2, w_new)
accuracy(y_test2, y_hat2)

for i in range(5):
    print(f"data {i+1}: y_hat2 = {y_hat2[i]}, y2= {y2[i]}")

"""Q6:"""

# محاسبه تعداد نمونه‌های هر کلاس
y_counts = df[['Class ']].value_counts()
# نمایش نمودار دایره‌ای
labels = ['Class 0', 'Class 1']
plt.pie(y_counts, labels=labels, autopct='%1.1f%%')
plt.axis('equal')  # این دستور برای اینه که دایره ما کامل باشد!

# افزودن تعداد داده‌های هر کلاس به صورت مستقیم بر روی نمودار
for i, count in enumerate(y_counts):
    plt.text(x=-0.92, y=-0.9 - i * 0.15, s=f'{labels[i]}: {count}', ha='center', fontsize=12)

plt.show()

! pip install -U imbalanced-learn

# undersampling :
from imblearn.under_sampling import RandomUnderSampler

y = pd.DataFrame(y, columns=[''])
rus = RandomUnderSampler(sampling_strategy=1)
x_res_undersampling , y_res_undersampling = rus.fit_resample(X , y)
ax = y_res_undersampling.value_counts().plot.pie(autopct = '%.2f')

"""Q7:"""

from sklearn.linear_model import LogisticRegression

X3 = shuffled_df[['Variance', 'Skewness', 'kurtosis', 'Entropy']].values
y3 = shuffled_df[['Class ']].values

x_train3, x_test3, y_train3, y_test3 = train_test_split(X3, y3, random_state=13, stratify=y3, test_size=0.2)

model = LogisticRegression(solver='sag', max_iter=210)
model.fit(x_train3, y_train3)
model.predict(x_test3)

print("Accuracy of test data: %", model.score(x_test3, y_test3)*100)
print("Accuracy of train data: %", model.score(x_train3, y_train3)*100)